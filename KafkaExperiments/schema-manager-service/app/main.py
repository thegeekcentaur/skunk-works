from fastapi import FastAPI, HTTPException, UploadFile, Form
from fastapi.responses import FileResponse
import requests
import os
import subprocess
import shutil
import tempfile
import json
from typing import Dict, Any

app = FastAPI()

SCHEMA_REGISTRY_URL = os.getenv("SCHEMA_REGISTRY_URL", "http://localhost:8081")
OUTPUT_DIR = "/app/output"
os.makedirs(OUTPUT_DIR, exist_ok=True)


def generate_python_class_from_schema(schema: Dict[str, Any]) -> str:
    """
    Generate Python dataclass code from Avro schema
    """
    schema_name = schema.get('name', 'Event')
    namespace = schema.get('namespace', 'com.example')
    fields = schema.get('fields', [])
    
    # Map Avro types to Python types
    type_mapping = {
        'string': 'str',
        'int': 'int',
        'long': 'int',
        'float': 'float',
        'double': 'float',
        'boolean': 'bool',
        'bytes': 'bytes',
        'null': 'None'
    }
    
    def get_python_type(avro_type):
        """Convert Avro type to Python type hint"""
        if isinstance(avro_type, str):
            return type_mapping.get(avro_type, 'Any')
        elif isinstance(avro_type, list):
            # Union type (e.g., ["null", "string"])
            non_null_types = [t for t in avro_type if t != 'null']
            if len(non_null_types) == 1:
                python_type = type_mapping.get(non_null_types[0], 'Any')
                return f'Optional[{python_type}]'
            return 'Any'
        elif isinstance(avro_type, dict):
            # Complex type (record, array, map, enum)
            avro_type_name = avro_type.get('type')
            if avro_type_name == 'array':
                items_type = get_python_type(avro_type.get('items'))
                return f'List[{items_type}]'
            elif avro_type_name == 'map':
                values_type = get_python_type(avro_type.get('values'))
                return f'Dict[str, {values_type}]'
            elif avro_type_name == 'enum':
                return 'str'  # Treat enums as strings
            return 'Any'
        return 'Any'
    
    # Generate imports
    code = '''"""
Auto-generated from Avro schema
DO NOT EDIT THIS FILE MANUALLY
"""
from dataclasses import dataclass, asdict
from typing import Optional, List, Dict, Any
import json

'''
    
    # Generate dataclass
    code += f'@dataclass\n'
    code += f'class {schema_name}:\n'
    code += f'    """Generated from Avro schema: {namespace}.{schema_name}"""\n\n'
    
    # Generate fields
    for field in fields:
        field_name = field['name']
        field_type = get_python_type(field['type'])
        field_doc = field.get('doc', '')
        
        if field_doc:
            code += f'    # {field_doc}\n'
        
        # Check if field has default value
        if 'default' in field:
            default_val = field['default']
            if default_val is None:
                code += f'    {field_name}: {field_type} = None\n'
            elif isinstance(default_val, str):
                code += f'    {field_name}: {field_type} = "{default_val}"\n'
            else:
                code += f'    {field_name}: {field_type} = {default_val}\n'
        else:
            code += f'    {field_name}: {field_type}\n'
        code += '\n'
    
    # Add helper methods
    code += '''    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for Kafka serialization"""
        return asdict(self)
    
    def to_json(self) -> str:
        """Convert to JSON string"""
        return json.dumps(self.to_dict())
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]):
        """Create instance from dictionary"""
        return cls(**data)
    
    @classmethod
    def from_json(cls, json_str: str):
        """Create instance from JSON string"""
        return cls.from_dict(json.loads(json_str))
'''
    
    return code


def generate_kafka_producer_code(schema: Dict[str, Any], package_name: str) -> str:
    """Generate Kafka producer wrapper code"""
    schema_name = schema.get('name', 'Event')
    topic_name = schema.get('name', 'events').lower()
    
    code = f'''"""
Kafka Producer for {schema_name}
Auto-generated - DO NOT EDIT
"""
from confluent_kafka import Producer
from confluent_kafka.serialization import SerializationContext, MessageField
from confluent_kafka.schema_registry import SchemaRegistryClient
from confluent_kafka.schema_registry.avro import AvroSerializer
import json
import os
from typing import Optional, Callable
from .models import {schema_name}


class {schema_name}Producer:
    """High-level Kafka producer for {schema_name} events"""
    
    def __init__(
        self,
        bootstrap_servers: Optional[str] = None,
        schema_registry_url: Optional[str] = None,
        topic: Optional[str] = None
    ):
        """
        Initialize producer
        
        Args:
            bootstrap_servers: Kafka bootstrap servers (default: from KAFKA_BOOTSTRAP_SERVERS env)
            schema_registry_url: Schema Registry URL (default: from SCHEMA_REGISTRY_URL env)
            topic: Kafka topic name (default: {topic_name})
        """
        self.bootstrap_servers = bootstrap_servers or os.getenv(
            'KAFKA_BOOTSTRAP_SERVERS', 'kafka:19092'
        )
        self.schema_registry_url = schema_registry_url or os.getenv(
            'SCHEMA_REGISTRY_URL', 'http://schema-registry:8081'
        )
        self.topic = topic or '{topic_name}'
        
        # Schema definition
        self.schema_str = json.dumps({json.dumps(schema, indent=8)})
        
        # Initialize Schema Registry client
        self.schema_registry_client = SchemaRegistryClient({{
            'url': self.schema_registry_url
        }})
        
        # Initialize Avro serializer
        self.avro_serializer = AvroSerializer(
            self.schema_registry_client,
            self.schema_str,
            lambda obj, ctx: obj.to_dict() if hasattr(obj, 'to_dict') else obj
        )
        
        # Initialize Kafka producer
        producer_conf = {{
            'bootstrap.servers': self.bootstrap_servers,
            'client.id': f'{schema_name.lower()}-producer'
        }}
        self.producer = Producer(producer_conf)
    
    @classmethod
    def create(cls):
        """Factory method to create producer with default configuration"""
        return cls()
    
    def send(
        self,
        event: {schema_name},
        key: Optional[str] = None,
        on_delivery: Optional[Callable] = None
    ):
        """
        Send event to Kafka asynchronously
        
        Args:
            event: {schema_name} instance to send
            key: Optional message key
            on_delivery: Optional delivery callback
        """
        try:
            # Serialize value
            serialized_value = self.avro_serializer(
                event,
                SerializationContext(self.topic, MessageField.VALUE)
            )
            
            # Send to Kafka
            self.producer.produce(
                topic=self.topic,
                key=key,
                value=serialized_value,
                on_delivery=on_delivery or self._default_delivery_report
            )
            
            # Trigger callbacks
            self.producer.poll(0)
            
        except Exception as e:
            print(f"Failed to send event: {{str(e)}}")
            raise
    
    def send_sync(self, event: {schema_name}, key: Optional[str] = None):
        """
        Send event synchronously and wait for confirmation
        
        Args:
            event: {schema_name} instance to send
            key: Optional message key
        """
        self.send(event, key)
        self.flush()
    
    def flush(self, timeout: float = 10.0):
        """
        Wait for all messages to be delivered
        
        Args:
            timeout: Maximum time to wait in seconds
        """
        self.producer.flush(timeout=timeout)
    
    def close(self):
        """Close producer and release resources"""
        self.producer.flush()
    
    @staticmethod
    def _default_delivery_report(err, msg):
        """Default callback for message delivery reports"""
        if err:
            print(f'Message delivery failed: {{err}}')
        else:
            print(f'Message delivered to {{msg.topic()}} [{{msg.partition()}}] @ {{msg.offset()}}')


# Example usage
if __name__ == "__main__":
    producer = {schema_name}Producer.create()
    
    # Create event
    event = {schema_name}(
        # Add your field values here
    )
    
    # Send event
    producer.send_sync(event)
    print("Event sent successfully!")
    producer.close()
'''
    
    return code


def generate_kafka_consumer_code(schema: Dict[str, Any], package_name: str) -> str:
    """Generate Kafka consumer wrapper code"""
    schema_name = schema.get('name', 'Event')
    topic_name = schema.get('name', 'events').lower()
    
    code = f'''"""
Kafka Consumer for {schema_name}
Auto-generated - DO NOT EDIT
"""
from confluent_kafka import Consumer, KafkaError
from confluent_kafka.serialization import SerializationContext, MessageField
from confluent_kafka.schema_registry import SchemaRegistryClient
from confluent_kafka.schema_registry.avro import AvroDeserializer
import json
import os
from typing import Optional, Callable
from .models import {schema_name}


class {schema_name}Consumer:
    """High-level Kafka consumer for {schema_name} events"""
    
    def __init__(
        self,
        group_id: str,
        bootstrap_servers: Optional[str] = None,
        schema_registry_url: Optional[str] = None,
        topic: Optional[str] = None
    ):
        """
        Initialize consumer
        
        Args:
            group_id: Consumer group ID
            bootstrap_servers: Kafka bootstrap servers (default: from KAFKA_BOOTSTRAP_SERVERS env)
            schema_registry_url: Schema Registry URL (default: from SCHEMA_REGISTRY_URL env)
            topic: Kafka topic name (default: {topic_name})
        """
        self.group_id = group_id
        self.bootstrap_servers = bootstrap_servers or os.getenv(
            'KAFKA_BOOTSTRAP_SERVERS', 'kafka:19092'
        )
        self.schema_registry_url = schema_registry_url or os.getenv(
            'SCHEMA_REGISTRY_URL', 'http://schema-registry:8081'
        )
        self.topic = topic or '{topic_name}'
        
        # Schema definition
        self.schema_str = json.dumps({json.dumps(schema, indent=8)})
        
        # Initialize Schema Registry client
        self.schema_registry_client = SchemaRegistryClient({{
            'url': self.schema_registry_url
        }})
        
        # Initialize Avro deserializer
        self.avro_deserializer = AvroDeserializer(
            self.schema_registry_client,
            self.schema_str
        )
        
        # Initialize Kafka consumer
        consumer_conf = {{
            'bootstrap.servers': self.bootstrap_servers,
            'group.id': self.group_id,
            'auto.offset.reset': 'earliest',
            'enable.auto.commit': True
        }}
        self.consumer = Consumer(consumer_conf)
        self.consumer.subscribe([self.topic])
        self.running = True
    
    @classmethod
    def create(cls, group_id: str):
        """Factory method to create consumer with default configuration"""
        return cls(group_id)
    
    def subscribe(self, handler: Callable[[{schema_name}], None]):
        """
        Subscribe to events and process with handler function
        
        Args:
            handler: Function that takes a {schema_name} instance
        """
        try:
            print(f"Starting consumer for topic: {{self.topic}}")
            
            while self.running:
                msg = self.consumer.poll(timeout=1.0)
                
                if msg is None:
                    continue
                
                if msg.error():
                    if msg.error().code() == KafkaError._PARTITION_EOF:
                        continue
                    else:
                        print(f"Consumer error: {{msg.error()}}")
                        continue
                
                try:
                    # Deserialize message
                    event_dict = self.avro_deserializer(
                        msg.value(),
                        SerializationContext(self.topic, MessageField.VALUE)
                    )
                    
                    # Convert to dataclass instance
                    event = {schema_name}.from_dict(event_dict)
                    
                    # Process event
                    handler(event)
                    
                except Exception as e:
                    print(f"Error processing message: {{str(e)}}")
                    
        except KeyboardInterrupt:
            print("Consumer interrupted by user")
        finally:
            self.close()
    
    def close(self):
        """Close consumer and release resources"""
        self.running = False
        self.consumer.close()
        print("Consumer closed")


# Example usage
if __name__ == "__main__":
    def process_event(event: {schema_name}):
        print(f"Received event: {{event}}")
        # Add your processing logic here
    
    consumer = {schema_name}Consumer.create("my-consumer-group")
    consumer.subscribe(process_event)
'''
    
    return code


@app.post("/schemas")
def submit_schema(entity_name: str = Form(...), schema_file: UploadFile = None):
    if not schema_file:
        raise HTTPException(400, "Schema file required")
    schema_content = schema_file.file.read().decode("utf-8")
    
    # Register to schema registry
    response = requests.post(
        f"{SCHEMA_REGISTRY_URL}/subjects/{entity_name}/versions",
        json={"schema": schema_content}
    )
    
    print(f"response: {response.text} ({response.status_code})")
    
    if response.status_code not in [200, 201]:
        raise HTTPException(400, f"Registration failed: {response.text}")
    
    return {"entity_name": entity_name, "id": response.json()["id"]}


@app.post("/schemas/{entity_name}/generate")
def generate_package(entity_name: str, version: int = Form(None), language: str = Form("python")):
    # Fetch latest or specific version
    url = f"{SCHEMA_REGISTRY_URL}/subjects/{entity_name}/versions/latest" if version is None \
        else f"{SCHEMA_REGISTRY_URL}/subjects/{entity_name}/versions/{version}"
    
    response = requests.get(url)
    if response.status_code != 200:
        raise HTTPException(404, "Schema not found")
    
    schema_str = response.json()["schema"]
    schema = json.loads(schema_str)
    schema_version = response.json()["version"]
    
    with tempfile.TemporaryDirectory() as tmpdir:
        package_name = f"{entity_name.replace('-', '_')}_lib"
        package_dir = os.path.join(tmpdir, package_name)
        os.makedirs(package_dir)
        
        if language == "python":
            print(f"Generating python-based package for {entity_name}")
            # Generate Python dataclass
            models_code = generate_python_class_from_schema(schema)
            with open(os.path.join(package_dir, "models.py"), "w") as f:
                f.write(models_code)
            print("    Created model class")
            # Generate Kafka producer
            producer_code = generate_kafka_producer_code(schema, package_name)
            with open(os.path.join(package_dir, "producer.py"), "w") as f:
                f.write(producer_code)
            print("    Created kafka producer")
            
            # Generate Kafka consumer
            consumer_code = generate_kafka_consumer_code(schema, package_name)
            with open(os.path.join(package_dir, "consumer.py"), "w") as f:
                f.write(consumer_code)
            print("    Created kafka consumer")
            
            # Add __init__.py
            schema_class_name = schema.get('name', 'Event')
            init_content = f'''"""
{package_name} - Auto-generated Kafka client library
Version: {schema_version}
"""

from .models import {schema_class_name}
from .producer import {schema_class_name}Producer
from .consumer import {schema_class_name}Consumer

__version__ = "{schema_version}"
__all__ = ["{schema_class_name}", "{schema_class_name}Producer", "{schema_class_name}Consumer"]
'''
            with open(os.path.join(package_dir, "__init__.py"), "w") as f:
                f.write(init_content)
            print("    Created __init__.py")
            
            # Setup.py for packaging
            with open(os.path.join(tmpdir, "setup.py"), "w") as f:
                f.write(f"""
from setuptools import setup, find_packages

setup(
    name='{package_name}',
    version='{schema_version}',
    packages=find_packages(),
    install_requires=[
        'confluent-kafka>=2.3.0',
        'avro-python3>=1.10.2',
    ],
    python_requires=">=3.7",
    description='Kafka client library for {schema.get("name", entity_name)} events',
    author='Auto-generated',
)
""")
            print("    Created setup.py")

            # Add README
            readme_content = f"""# {package_name}

Auto-generated Kafka client library for {schema.get('name', entity_name)} events.

## Installation

```bash
pip install {package_name}-{schema_version}-py3-none-any.whl
```

## Usage

### Producer

```python
from {package_name} import {schema_class_name}, {schema_class_name}Producer

# Create producer
producer = {schema_class_name}Producer.create()

# Create event
event = {schema_class_name}(
    {', '.join(f'{field["name"]}="value"' for field in schema.get("fields", [])[:2])}
)

# Send event
producer.send_sync(event)
producer.close()
```

### Consumer

```python
from {package_name} import {schema_class_name}Consumer

def handle_event(event):
    print(f"Received: {{event}}")

consumer = {schema_class_name}Consumer.create("my-group")
consumer.subscribe(handle_event)
```
"""
            with open(os.path.join(tmpdir, "README.md"), "w") as f:
                f.write(readme_content)
            print("    Added README")

            # Build wheel
            result = subprocess.run(
                ["python3", "setup.py", "bdist_wheel"],
                cwd=tmpdir,
                capture_output=True,
                text=True
            )
            print("    Built wheel")

            if result.returncode != 0:
                print(f"Build error: {result.stderr}")
                return HTTPException(422, f"An error occurred during build. {result.stderr}")
            
            dist_dir = os.path.join(tmpdir, "dist")
            if not os.path.exists(dist_dir):
                raise HTTPException(503, "Failed to build package - dist directory not found")
            
            wheel_files = [p for p in os.listdir(dist_dir) if p.endswith(".whl")]
            if not wheel_files:
                print("Failed to build package - no wheel file generated")
                raise HTTPException(503, "Failed to build package - no wheel file generated")
            
            wheel_path = os.path.join(dist_dir, wheel_files[0])
            
            # Copy to persistent directory
            output_filename = f"{package_name}-{schema_version}-py3-none-any.whl"
            output_path = os.path.join(OUTPUT_DIR, output_filename)
            shutil.copy2(wheel_path, output_path)
            print(f"    Copied {wheel_path} to {output_path}")
            
            return FileResponse(
                path=output_path,
                filename=output_filename,
                media_type='application/octet-stream'
            )
        
        else:
            raise HTTPException(503, f"{language} is not supported yet")


# Optional: Cleanup endpoint to remove old generated files
@app.delete("/packages/{filename}")
def cleanup_package(filename: str):
    """Delete a generated package file"""
    file_path = os.path.join(OUTPUT_DIR, filename)
    if os.path.exists(file_path):
        os.remove(file_path)
        return {"message": f"Deleted {filename}"}
    raise HTTPException(404, "File not found")


@app.get("/packages")
def list_packages():
    """List all generated packages"""
    if not os.path.exists(OUTPUT_DIR):
        return {"packages": []}
    
    packages = []
    for filename in os.listdir(OUTPUT_DIR):
        file_path = os.path.join(OUTPUT_DIR, filename)
        packages.append({
            "filename": filename,
            "size": os.path.getsize(file_path),
            "created": os.path.getctime(file_path)
        })    
    return {"packages": packages}
